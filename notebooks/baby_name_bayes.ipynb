{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "b2969e560496bd8105480ba5dac81391",
     "grade": false,
     "grade_id": "cell-3baa72d698526cb5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h1>Bayesian Modeling</h1>\n",
    "\n",
    "<p>This project uses naive bayes to build a baby name classifier. The classifier will use the features commonly found in names to distinguish whether a name is likely to be given to a baby girl or boy.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "dbf63747338c3bc801a52729939be2b7",
     "grade": false,
     "grade_id": "cell-714117928030f1f1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Python Initialization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "checksum": "9b7875183d29e111c4447b054bb724a4",
     "grade": false,
     "grade_id": "cell-18a2bc2f691a85d9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "73d7ace595c636f6af1c0c54d9091952",
     "grade": false,
     "grade_id": "cell-39c6afaee8154fa0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>The <code>hash_features</code> and <code>names2features</code> functions:</h3>\n",
    "<p>Below, the <code>hash_features</code> and <code>names2features</code> functions will take the plain text names and convert them to feature vectors to work with the data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "checksum": "b10d2fe91c6cf8c951e25fb196052154",
     "grade": false,
     "grade_id": "cell-1e080d9347c9d997",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def hash_features(baby: str, b: int, fix: int):\n",
    "    \"\"\"\n",
    "    This function takes a baby name and converts it into a feature vector.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    baby : a string representing the baby's name to be hashed\n",
    "    b    : the number of dimensions to be in the feature vector\n",
    "    fix  : the number of chunks to extract and hash from each string\n",
    "    \n",
    "    Outputs:\n",
    "    -------\n",
    "    v : a feature vector representing the input string\n",
    "    \"\"\"\n",
    "    v = np.zeros(b)\n",
    "    for m in range(fix):\n",
    "        feature_string = 'prefix' + baby[:m]\n",
    "        v[hash(feature_string) % b] = 1\n",
    "        feature_string = 'suffix' + baby[-m:]\n",
    "        v[hash(feature_string) % b] = 1\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "checksum": "a1bfccfe5e69c2936d946537fc9fbbb0",
     "grade": false,
     "grade_id": "cell-1809ceffa163f422",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def names2features(filename: str, b: int=128, fix: int=3, usefile: bool=True):\n",
    "    \"\"\"\n",
    "    This function takes a file of baby names and generates feature vectors.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    filename : the filepath string or names to use\n",
    "    b        : the number of dimensions to be in the feature vector\n",
    "    fix      : the number of chunks to extract and hash from each string\n",
    "    usefile  : whether to load a file or use a name directly\n",
    "    \n",
    "    Outputs:\n",
    "    -------\n",
    "    x : n feature vectors of dimension (nxb)\n",
    "    \"\"\"\n",
    "    if usefile:\n",
    "        with open(filename, 'r') as f:\n",
    "            baby_names = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        baby_names = filename.split('\\\\n')\n",
    "    \n",
    "    n = len(baby_names)\n",
    "    x = np.zeros((n, b))\n",
    "    for i in range(n):\n",
    "        x[i,:] = hash_features(baby_names[i], b, fix)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "6d3d3cdfda91d8dabf6c2a98d57b87ea",
     "grade": false,
     "grade_id": "cell-844dbe02c2f5bdc3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<p>In the code cell above, <code>names2features</code> reads every name in the given file and converts it into a 128-dimensional feature vector by first assembling substrings (based on the parameter 'fix'), then hashing these assembled substrings and modifying the feature vector index (the modulo of the number of dimensions) that corresponds to this hash value.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "39280620b8cbeb29243b217c8a785fcb",
     "grade": false,
     "grade_id": "cell-e96f9a3b1bc1cbe9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>The <code>generate_features</code> function:</h3>\n",
    "<p>Below is a python function <code>generate_features</code>, which transforms the names into features and loads them into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "checksum": "11513197aef0a07a5672f0439274c519",
     "grade": false,
     "grade_id": "cell-0b3fe702eadc1daa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_features(dim: int=128):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    ------\n",
    "    dim : desired dimension of the features\n",
    "        \n",
    "    Outputs:\n",
    "    -------\n",
    "    X : n feature vectors of dimensionality d (nxd)\n",
    "    y : n labels (-1 = girl, +1 = boy) (n)\n",
    "    \"\"\"\n",
    "    girl_names = names2features('../data/girls_train.csv', b=dim)\n",
    "    boy_names = names2features('../data/boys_train.csv', b=dim)\n",
    "    \n",
    "    X = np.concatenate([girl_names, boy_names])\n",
    "    y = np.concatenate([-np.ones(len(girl_names)), np.ones(len(boy_names))])\n",
    "    \n",
    "    ii = np.random.permutation([i for i in range(len(y))])\n",
    "    \n",
    "    return X[ii,:], y[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "6601f8020f99537f8a4156b4837302da",
     "grade": false,
     "grade_id": "cell-8ec662b973a6ef03",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>The Na&iuml;ve Bayes Classifier</h2>\n",
    "\n",
    "<p>The Na&iuml;ve Bayes classifier is a linear classifier based on Bayes Rule. The following cells walk through the steps necessary to properly implement it.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "51c6d345ce0432218bd94a7ee3ae56c5",
     "grade": false,
     "grade_id": "cell-e91e263ce575ecdf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part One: Class Probability</h3>\n",
    "\n",
    "<p>Estimate the class probability $P(y)$ in \n",
    "<b><code>naive_bayes_prior</code></b>. This returns the probability that a sample in the training set is positive or negative, independent of its features.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "checksum": "8b9b66ef6549826bae77cb5416e97dc0",
     "grade": false,
     "grade_id": "cell-naivebayesPY",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def naive_bayes_prior(X, y):\n",
    "    \"\"\"\n",
    "    This function calculates the prior probability.\n",
    "    This is the probability that a sample in the training\n",
    "    set is positive or negative, independent of the features.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    X : n input vectors of d dimensions (nxd)\n",
    "    y : n labels (-1 or +1) (nx1)\n",
    "    \n",
    "    Outputs:\n",
    "    -------\n",
    "    pos : probability P(y=1)\n",
    "    neg : probability P(y=-1)\n",
    "    \"\"\"\n",
    "    # plus-one smoothing\n",
    "    y = np.concatenate([y, [-1, 1]])\n",
    "    n = len(y)\n",
    "    pos = sum(y == 1) / n\n",
    "    \n",
    "    return pos, 1 - pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "92556d6a657de6b831a6ff25e47bb921",
     "grade": false,
     "grade_id": "cell-e3d763d037cc430a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Two: Conditional Probability</h3>\n",
    "\n",
    "<p>Estimate the conditional probabilities $P([\\mathbf{x}]_{\\alpha}|y)$ in \n",
    "<b><code>naive_bayes_conditional</code></b>. Notice that by construction, the features are binary categorical features. This uses a <b>categorical</b> distribution and return the probability vectors for each feature being 1 given a class label. Note that the result will be two vectors of length d (the number of features), where the values represent the probability that feature i is equal to 1.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "checksum": "16643b0595cc048eeda2613d8e14ec25",
     "grade": false,
     "grade_id": "cell-naivebayesPXY",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def naive_bayes_conditional(X, y):\n",
    "    \"\"\"\n",
    "    This function computes the conditional probability of each feature.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    X : n input vectors of d dimensions (nxd)\n",
    "    y : n labels (-1 or +1) (n)\n",
    "    \n",
    "    Outputs:\n",
    "    -------\n",
    "    pos_prob : probability vector of p(x_alpha = 1 | y = 1) (d)\n",
    "    neg_prob : probability vector of p(x_alpha = 1 | y = -1) (d)\n",
    "    \"\"\"\n",
    "    # plus-one smoothing\n",
    "    n, d = X.shape\n",
    "    X = np.concatenate([X, np.ones((2, d)), np.zeros((2, d))])\n",
    "    y = np.concatenate([y, [-1, 1, -1, 1]])\n",
    "    \n",
    "    pos = X[y == 1]\n",
    "    neg = X[y == -1]\n",
    "    \n",
    "    pos_count = len(pos)\n",
    "    neg_count = len(neg)\n",
    "    \n",
    "    pos_feature_count = np.sum(pos, axis=0)\n",
    "    neg_feature_count = np.sum(neg, axis=0)\n",
    "    \n",
    "    pos_prob = pos_feature_count / pos_count\n",
    "    neg_prob = neg_feature_count / neg_count\n",
    "    \n",
    "    return pos_prob, neg_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "04955830c69bbd778338d8cd362fa6f3",
     "grade": false,
     "grade_id": "cell-718eb20a3bd07d46",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Three: Log Likelihood</h3>\n",
    "\n",
    "<p>Calculate the log likelihood $\\log P(\\mathbf{x}|y)$ for each point in X_test in \n",
    "<b><code>loglikelihood</code></b> and label y_test. The likelihood is given by the product of the conditional probabilities of each feature and that $\\log(ab) = \\log a + \\log b$.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "checksum": "1db38bb538470522ee352acbfbacf359",
     "grade": false,
     "grade_id": "cell-loglikelihood",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loglikelihood(pos_prob, neg_prob, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This function calculates the loglikelihood of each point.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    pos_prob : conditional probabilities for the positive class (d)\n",
    "    neg_prob : conditional probabilities for the negative class (d)\n",
    "    X_test   : features (nxd)\n",
    "    y_test   : labels (-1 or +1) (n)\n",
    "    \n",
    "    Outputs:\n",
    "    -------\n",
    "    loglikelihood : loglikelihood of each point in X_test (n)\n",
    "    \"\"\"\n",
    "    n, d = X_test.shape\n",
    "    loglikelihood = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        x = X_test[i,:]\n",
    "        y = y_test[i]\n",
    "        if y == 1:\n",
    "            ll = np.sum(x * np.log(pos_prob) + (1 - x) * np.log(1 - pos_prob))\n",
    "        else:\n",
    "            ll = np.sum(x * np.log(neg_prob) + (1 - x) * np.log(1 - neg_prob))\n",
    "            \n",
    "        loglikelihood[i] = ll\n",
    "        \n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "7b077b79206b3d18b78c952cb4b76fd4",
     "grade": false,
     "grade_id": "cell-939ef69834417c3a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Part Four: Naive Bayes Prediction</h3>\n",
    "\n",
    "<p>For a test point $\\mathbf{x}_{test}$, we should classify it as positive if the log ratio $\\log\\left(\\frac{P(y=1 | \\mathbf{x} = \\mathbf{x}_{test})}{P(y=-1|\\mathbf{x} = \\mathbf{x}_{test})}\\right) > 0$ and negative otherwise. <b><code>naive_bayes_prediction</code></b> first calculates the log ratio $\\log\\left(\\frac{P(y=1 | \\mathbf{x} = \\mathbf{x}_{test})}{P(y=-1|\\mathbf{x} = \\mathbf{x}_{test})}\\right)$ for each test point in $\\mathbf{x}_{test}$ using Bayes' rule and predicts the label of the test points by looking at the log ratio.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "checksum": "7e74c5b2b25acdb08dd6b3b2ce1a8d35",
     "grade": false,
     "grade_id": "cell-naivebayes_pred",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def naive_bayes_prediction(pos, neg, pos_prob, neg_prob, X_test):\n",
    "    \"\"\"\n",
    "    This function returns the prediction of each provided point.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    pos      : class probability for the positive class\n",
    "    neg      : class probability for the negative class\n",
    "    pos_prob : conditional probabilities for the positive class (d)\n",
    "    neg_prob : conditional probabilities for the negative class (d)\n",
    "    X_test   : features (nxd)\n",
    "    \n",
    "    Outputs:\n",
    "    -------\n",
    "    preds : prediction of each point in X_test (n)\n",
    "    \"\"\"\n",
    "    n, d = X_test.shape\n",
    "    num = loglikelihood(pos_prob, neg_prob, X_test, np.ones(n)) + np.log(pos)\n",
    "    den = loglikelihood(pos_prob, neg_prob, X_test, -np.ones(n)) + np.log(neg)\n",
    "    \n",
    "    p = num - den\n",
    "    preds = -np.ones(n)\n",
    "    preds[p > 0] = 1\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "47abd4ba6ad4cb0f1c6446cd6c46afbd",
     "grade": false,
     "grade_id": "cell-d31d3867bef12ca7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we can test the code with the following interactive name classification script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "checksum": "f54bd93d468d1ac66303ad42f1b34a50",
     "grade": false,
     "grade_id": "cell-a53cd34f0d01fec0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training classifier ...\n",
      "Testing classifier ...\n",
      "Training error: 28.06%\n"
     ]
    }
   ],
   "source": [
    "print('Loading data ...')\n",
    "X, y = generate_features(dim=128)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print('Training classifier ...')\n",
    "pos, neg = naive_bayes_prior(X_train, y_train)\n",
    "pos_prob, neg_prob = naive_bayes_conditional(X_train, y_train)\n",
    "\n",
    "print('Testing classifier ...')\n",
    "prediction = naive_bayes_prediction(pos, neg, pos_prob, neg_prob, X_test)\n",
    "error = np.mean(prediction != y_test)\n",
    "print('Training error: %.2f%%' % (100 * error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a baby name>\n",
      "Sarah\n",
      "Sarah, I am sure you are a baby girl.\n",
      "\n",
      "Please enter a baby name>\n",
      "John\n",
      "John, I am sure you are a baby boy.\n",
      "\n",
      "Please enter a baby name>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print('Please enter a baby name>')\n",
    "    name = input()\n",
    "    if len(name) < 1:\n",
    "        break\n",
    "    \n",
    "    x = names2features(name, b=128, usefile=False)\n",
    "    prediction = naive_bayes_prediction(pos, neg, pos_prob, neg_prob, x)\n",
    "    if prediction > 0:\n",
    "        print(\"%s, I am sure you are a baby boy.\\n\" % name)\n",
    "    else:\n",
    "        print(\"%s, I am sure you are a baby girl.\\n\" % name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
